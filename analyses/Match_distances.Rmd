---
title: "Distance Simulations"
author: "Rachael Caelie (Rocky) Aikens"
date: "7/11/2019"
output: pdf_document
---

```{r setup, warning=FALSE, message = FALSE, include = FALSE}
knitr::opts_chunk$set(cache=TRUE, warning = FALSE, message = FALSE, echo = FALSE, fig.align = "center", fig.height = 4)
require(dplyr)
require(ggplot2)
require(gridExtra)
require(ggpubr)
require(knitr)
theme_set(theme_light())
source("../code/basic_sim_functions.R")
source("../code/distance_sim_functions.R")
```

# Set Up 

As a different measure of performance, we chose to consider the distance between matched pairs in the propensity-by-prognostic feature reduction space shown in figure 1 of the manuscript.  For this analysis, we simulated data sets of varying size and measured the mean Mahalanobis distance (in the prognosis-propensity feature space) between matched observations for Buffalo, Propensity, and Mahalanobis Distance matching. The generative model for all of our simulations is the following:
\begin{align*}
    X_i &\sim_{iid} \text{Normal}(0,I_p),\\
    T_i &\sim_{iid} \text{Bernoulli}\left(\frac{1}{1+\exp(-\phi(X_i))}\right),\\
    Y_i &=\tau T_i + \Psi(X_i) + \epsilon_i,\\
    \epsilon_i &\sim_{iid} N(0,\sigma^2),
\end{align*}
where the true propensity and prognositic scores are given by the linear combinations
\begin{align*}
    \phi(X_i) &= X_{i1}/3-c,\\
    \Psi(X_i) &=\rho X_{i1} + \sqrt{(1-\rho^2)}X_{i2},
\end{align*}
so that $\text{Cor}(\phi(X_i), \Psi(X_i)) \propto \rho$.  The constant $c$ in the propensity score formula was chosen such that there were approximately 100 treated observations in each dataset. We consider $p=10$.  Unlike in the set-up for other simulations, we consider here only $\rho = 0.5$ and $k=3$. Each simulation consisted of a dataset of size $n=2000, 1600, 1000$ and was repeated $N=1000$ times.
We fix the treatment effect to be constant with $\tau=1$ and the noise to be $\sigma=1$.
For a given matching, we estimate ATT and design sensitivity $\tilde\Gamma$ using the permutation $t$-statistic from the package `sensitivtymv`.


\pagebreak 

# Results

## Main Simulations: Mahalanobis distance in Propensity x Prognosis

First, I measured the average mahalanobis distance between treated and control individuals in the reduced feature space of prognostic score by propensity score.  I was expecting that prognostic methods would perform better when sample size was large, and mahalanobis methods would perform better when sample size was small.  Surprisingly, I found that Mahalanobis was almost always the most effective.  Below is a table of the mean distance between matched pairs in terms of the true prognosis and log propensity.

```{r}
read_data_1000 <- function(i, path_to_file){
  filename <- paste(path_to_file, "dist_results_5_",i,"_1000", sep = "")
  dat <- read.csv(filename) %>%
    mutate(k = 3, rho = 0.5, n = i)
  return(dat)
}
```

```{r}
Ns <- c(2000, 1800, 1600, 1400, 1200, 1000)

dat <- lapply(Ns, function(x) read_data_1000(x, "../data/dist_results/")) %>% bind_rows
```


```{r}
rho = 0.5
kable(dat %>%
  select(-c(k, rho)) %>%
  group_by(method, n) %>%
  summarize(mean_dist = mean(mean_dist)) %>%
  ungroup() %>%
  spread(method, mean_dist) %>%
  arrange(-n))
```

Here, you can see a histogram of the mean distances across simulations.  Mahalanobis consistently has a lower mean distance.

```{r}
ggplot(filter(dat, n == 2000), aes(x = mean_dist, group = method, fill = method)) + geom_histogram(aes(alpha = 0.2), position = "identity") + xlab("Mean Mahalanobis dist in psi x phi")
```

\pagebreak

## Potential Explanation? Model fitting 

It may be that one of the reasons Buffalo and propensity score matching are selecting more distant matches on average is that the prognostic and propensity models are difficult to fit.  For example, we can see in simulation that a lot of the drop in performance is eliminated when you assume the true model is given.

Below, we show Figure 1 of the Buffalo manuscript, where we find optimal matchings given the true scores:

```{r}
match_viz <- function(data, match, rho, k = 1, title = "Matching"){
  plt_data <- data %>% 
    mutate(m = match) %>%
    mutate(a = ifelse (is.na(m), 0.9, 1)) %>% 
    mutate(prog = rho*X1 + sqrt(1-rho^2)*X2, 
           prop = mu,
           t = as.factor(abs(1-t))) %>%
    select(c(t, prog, prop, m, a))
  
  m_data <- plt_data %>% 
    filter(!is.na(m)) %>%
    arrange(m, desc(t)) %>% 
    mutate(id = rep(1:(k + 1), sum(data$t))) %>%
    select(-c(t, a)) %>%
    group_by(m) %>%
    summarize(prop1 = first(prop), prop2 = last(prop),
              prog1 = first(prog), prog2 = last(prog)) %>%
    select(prog1, prog2, prop1, prop2)
  
  plt <- ggplot(data = plt_data, aes( x = prop, y = prog, group = t, color = t)) + 
    geom_point(aes(alpha = a))+
    scale_color_brewer(palette="Set1") +
    geom_segment(data = m_data, 
                 aes(x = prop1, y = prog1,
                     xend = prop2, yend = prog2),
                 color =  "black", group = NA, linetype = "dashed") +
    ggtitle( title)+
    theme(legend.position = "none", aspect.ratio=1, plot.title = element_text(hjust = 0.5, size = 8))+
    ylab(expression(paste(Psi, "(x)", sep = ""))) +
    xlab(expression(paste(phi, "(x)", sep = "")))
  
  return(plt)
}

# like prognostic match except returns data frame and match assignments, not just the
# reformatted dataframe of outcomes by match assignment
prognostic_match_assignment <- function(df, propensity, match_assignment, prog_model, n_control) {
  df$m <- match_assignment
  df$row <- 1:nrow(df)
  n_t<- sum(df$t)

  selected <- df %>% 
    filter(!is.na(m)) %>%
    filter(t==0) %>%
    group_by(m) %>%
    sample_n(size = 1)
  
  prognostic <- lm(prog_model, data = selected)
  not_selected <- df[-selected$row, ]
  not_selected <- not_selected %>% 
			mutate(progscore = predict(prognostic, not_selected)) %>%
			mutate(propscore = predict(propensity, not_selected))
  prog_dist <- match_on(t ~ progscore + propscore, data = not_selected)
  prog_match <- pairmatch(prog_dist, controls = n_control, data = not_selected) 
  return(list(df = not_selected, match = prog_match, k = n_control))
}
```

```{r}
rho <- 0.5
#simulate data
df <- generate_data(N = 2000, p = 10, true_mu = "X1/3-3", rho = rho, sigma = 1)
k = 1
prop_model = formula(t ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10)
prog_model = formula(y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10)

# mahalanobis match
mahal_dist <- match_on(prop_model, method = "mahalanobis", data = df)
m_match <- pairmatch(mahal_dist, controls = k, df)
```


```{r}
#Calculate true propensity and prognostic score, and match on the true score
oracle_df <- df %>% 
    mutate(prog = rho*X1 + sqrt(1-rho^2)*X2, 
           prop = 1/(1+exp(-(mu))))

oracle_prop_match <- pairmatch(t ~ prop, controls = k, oracle_df)
oracle_prog_match <- pairmatch(t ~ prog + prop, controls = k, oracle_df)
```

```{r, fig.width=8.5, fig.height= 3.5}
a <- match_viz(df, m_match, rho, title = "Mahalanobis Match")
b <- match_viz(df, oracle_prop_match, rho, title = "True Propensity Match")
c <- match_viz(df, oracle_prog_match, rho, title = "True Propensity x Prognosis Match")

ggarrange(a,b,c, ncol= 3, labels = "AUTO" )
```

```{r}
oracle_dists <- c(true_mean_dist(df, m_match, rho), true_mean_dist(df, oracle_prop_match, rho),  true_mean_dist(df, oracle_prog_match, rho))
```

In contrast, here are the matches that we might pick if we had to estimate the propensity and prognostic models ourselves rather than relying on an oracle.

```{r}
# Build scores empirically for propensity and prognostic match

# build propensity score
propensity <- glm(prop_model, family = binomial(), data = df)
  
prop_match <- pairmatch(propensity, controls = k, df)
  
# 1:2 mahalanobis matching to select data to use for prognostic model
mahal_match <- pairmatch(mahal_dist, controls = 2, df) 
  
buff_match_assignment <- prognostic_match_assignment(df, propensity, mahal_match, prog_model, k)
```

```{r, fig.width=8.5, fig.height= 3.5}
a <- match_viz(df, m_match, rho, title = "Mahalanobis Match")
b <- match_viz(df, prop_match, rho, title = "True Propensity Match")
c <- match_viz(buff_match_assignment$df, buff_match_assignment$match, rho, title = "True Propensity x Prognosis Match")

ggarrange(a,b,c, ncol= 3, labels = "AUTO" )
```

The table below shows the avrage distances between matched pairs assuming (Oracle) the true propensity and prognostic scores are provided by an oracle, and (Estimated) the propensity and prognostic scores are estimated.
```{r}
empirical_dists <- c(true_mean_dist(df, m_match,rho), true_mean_dist(df, prop_match, rho), true_mean_dist(buff_match_assignment$df, buff_match_assignment$match, rho))

tab <- rbind(oracle_dists, empirical_dists)
colnames(tab) <- c("Mahalanobis", "Propensity", "Buffalo")
rownames(tab) <- c("Oracle", "Estimated")

kable(tab, digits = 2)
```


## Smaller Simulations

I tried to better understand this with smaller simulations.  From this point onward, all results are the product of N=30 simulations.

### Estimates

As a sanity check, I looked at the correspondence between mean distance and estimate for a smaller batch of simulations with the same parameters.  They are not closely correlated.  Prognostic methods tend to be closer to the correct estimate than Mahalanobis, in spite of the fact that Mahalanobis matches are closer in the reduced feature space

```{r}
set.seed(123)
k <- 3
nsim <- 30
results <- replicate(nsim, simulate_for_distances(generate_data(N=2000, rho=0.5, p = 10, true_mu = "X1/3-3"),
                                        verbose = TRUE, k = k, true_rho = 0.5, gamma = TRUE),
                         simplify = FALSE) %>% 
      bind_rows()
```

```{r}
ggplot(data = results, aes(x = mean_dist, y = abs(estimate-1), color = method, group = method)) + geom_point()+ xlim(0,2) + ylim(0,0.5)+ xlab("Mean Mahalanobis dist in psi x phi") + ylab("Absolute Value Error")
```

### Estimated distances

As a further sanity check, I wanted to make sure propensity and prognostic models were performing well on the estimated propensity and prognostic scores.  They are; buffalo matching at least thinks it's doing a great job.

```{r}
kable(results %>% group_by(method) %>% summarize("Mean Estimated Distance"= mean(emp_mean_dist)))
```

### Absolute propensity and prognostic differences

Next, I asked what the distances were between matches sets just in terms of log propensity or just in terms of prognosis.

```{r}
kable(results %>% group_by(method) %>% summarise("Prognostic distance" = mean(prog_dist), "Log propensity distance" = mean(prop_dist)))
```

What's most surprising to me here is that propensity score matching actually does worst at minimizing log propensity distances.  What?

I can plot these distances together like so:

```{r}
ggplot(results, aes(x = prog_dist, y = prop_dist, group = method, color = method)) + geom_point() + xlim(0,1.25) + ylim(0,0.5)+ xlab("Mean prognostic distance") + ylab("Mean propensity distance")
```

A couple observations here:

- Buffalo matching does better than all other methods at minimizing prognostic differences.

- Mahalanobis is most consistent.

- Propensity score matching is doing far worse than I would expect at minimizing propensity score distances.

Below, I set the size of the points so that estimates closer to the true effect are larger.  Somehow propensity score matching does well even though the distances between matched pairs are the worst.

```{r}
ggplot(results, aes(x = prog_dist, y = prop_dist, group = method, color = method, size = 1/abs(estimate -1 ))) + geom_point() + xlim(0,1.25) + ylim(0,0.75)+ xlab("Mean prognostic distance") + ylab("Mean propensity distance")
```

### Gamma

I also ran simulations which measured gamma design sensitivity.  The plots below show propensity and prognostic distances versus gamma.  I overlayed a linear model on this so that you can see the trends between prognostic balance and gamma.  I hypothesize that the positive correllation between gamma and prognostic imbalance for mahalanobis distance reflects the fact that mahalanobis matchings give biased estimates, which increase gamma in an unconstructive manner; increasingly poor matches give increasing bias which gives increasing gamma.

```{r}
ggplot(results, aes(x = prog_dist, y = gamma, group = method, color = method)) + geom_point() + xlim(0,1.25) + xlab("Mean prognostic distance") + ylab("Gamma") + geom_smooth(method = "lm", se = FALSE)
```


Below is the same chart but with propensity distance on the x axis.
```{r}
ggplot(results, aes(x = prop_dist, y = gamma, group = method, color = method)) + geom_point() + xlim(0,0.5) + xlab("Mean propensity distance") + ylab("Gamma")
```


### Tinkering with simulation parameters

#### Less randomness in treatment assignment

For reasons I don't understand, running the simulations with true log propensity "X1-10/3" rather than "X1/3-3" changes things.  Below, we see that buffalo matching is now finding better matches than mahalanobis.  This is especially interesting because we found that, in terms of MSE, Buffalo matching doesn't do as well with log propensity "X1-10/3".  In essense, switching to log propensity "X1-10/3" means that treatment assignment is more systematic and less random than in our usual simulations.  Below are some of the same diagnostic plots as above, with the altered simulation parameters.

```{r}
set.seed(123)
k <- 3
results_mu <- replicate(nsim, simulate_for_distances(generate_data(N=2000, rho=0.5, p = 10, true_mu = "X1-10/3"),
                                        verbose = TRUE, k = k, true_rho = 0.5),
                         simplify = FALSE) %>% 
      bind_rows()
```

```{r}
ggplot(data = results_mu, aes(x = mean_dist, y = abs(estimate-1), color = method, group = method)) + geom_point()+ xlim(0,2) + ylim(0,0.5)+ xlab("Mean Mahalanobis dist in psi x phi") + ylab("Absolute Value Error")
```

```{r}
ggplot(results_mu, aes(x = prog_dist, y = prop_dist, group = method, color = method, size = 1/abs(estimate -1 ))) + geom_point() + xlim(0,1.25) + ylim(0,0.75)+ xlab("Mean prognostic distance") + ylab("Mean propensity distance")
```

#### More noise variables

I also reran the simulations with p = 50 features instead of p = 10. Increasing the number of features makes things more difficult for all methods.  Mahalanobis has more noise covariates to match on, but propensity and prognostic methods have to deal with a more complex feature space reduction.  As p increases, the models we fit for propensity and prognosis become increasingly overspecified.

```{r}
set.seed(123)
k <- 3
results_p <- replicate(nsim, simulate_for_distances(generate_data(N=2000, rho=0.5, p = 25, true_mu = "X1/3-3"),
                                        verbose = TRUE, k = k, true_rho = 0.5),
                         simplify = FALSE) %>% 
      bind_rows()
```

```{r}
ggplot(data = results_p, aes(x = mean_dist, y = abs(estimate-1), color = method, group = method)) + geom_point()+ xlim(0,2) + ylim(0,0.5)+ xlab("Mean Mahalanobis dist in psi x phi") + ylab("Absolute Value Error")
```

```{r}
ggplot(results_p, aes(x = prog_dist, y = prop_dist, group = method, color = method, size = 1/abs(estimate -1 ))) + geom_point() + xlim(0,1.25) + ylim(0,0.75)+ xlab("Mean prognostic distance") + ylab("Mean propensity distance")
```

# Distance and bias

Distance is interesting, but it's also worth thinking about systematic deviations in score: who (treated or control) tends to have the higher score and who has the lower one?  If the treated individuals systematically have the higher prognostic score in certain scenarios, that tells us something.  This was something that one of our reviewers asked about.

In essence, if $i_1$ and $i_0$ are the indices of a pair of treated and control individuals, we want to know $E\left[\Psi(X_{i_1}) - \Psi(X_{i_0})\right]$.  For our study to be unbaised, we'd like this quantity to be as low as possible.  The plots below answer that question, assuming that we had an oracle deliver the correct score models to us so that we didn't have to fit them ourselves.

```{r}
source("../code/SITA_violation_sim_functions.R")

get_mean_diff <- function(df, mymatch, rho, nu){
  # reformat
  plt_data <- df %>% 
    mutate(m = mymatch) %>%
    mutate(a = ifelse (is.na(m), 0.9, 1)) %>% 
    mutate(prog = rho*X1 + sqrt(1-rho^2)*X2 + nu*U, 
           prop = mu,
           t = as.factor(abs(1-t))) %>%
    dplyr::select(c(t, prog, prop, m, a))
  
  m_data <- plt_data %>% 
    filter(!is.na(m)) %>%
    arrange(m, desc(t)) %>% 
    mutate(id = rep(1:2, sum(df$t))) %>%
    dplyr::select(-c(t, a)) %>%
    group_by(m) %>%
    summarize(prop1 = first(prop), prop2 = last(prop),
              prog1 = first(prog), prog2 = last(prog)) %>%
    dplyr::select(prog1, prog2, prop1, prop2)
  
  result <- m_data %>% summarize(diff = mean(prog1 - prog2))
  return(result[[1,1]])
}

simulate_diff <- function(method, rho, nu = 0, prog_formula = as.formula(y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10),
                          prop_formula = as.formula(t ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10), 
                          mahal_formula = as.formula(t ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10)){

  df <- generate_xSITA_data(N = 2000, p = 10, true_mu = "X1/3-3", rho = rho, nu = nu, sigma = 1)
  propensity <- glm(prop_model, family = binomial(), data = df)
  
  if(method == "Mahalanobis"){
     mymatch <- pairmatch(mahal_formula, controls = 1, df) 
  }
  if(method == "Propensity"){
    mymatch <- pairmatch(propensity, controls = 1, df)
  }
  if(method == "Pilot"){
    # 1:2 mahalanobis matching to select data to use for prognostic model
    mahal_match <- pairmatch(mahal_dist, controls = 2, df) 
  
    buff_match_assignment <- prognostic_match_assignment(df, propensity, mahal_match, prog_formula, 1)
    df <- buff_match_assignment$df
    mymatch <- buff_match_assignment$match
  }
  return(get_mean_diff(df, mymatch, rho, nu))
}
```

```{r}
mahal_mean_difs <- replicate(100, simulate_diff("Mahalanobis", 0.5)) 
prog_mean_difs <- replicate(100, simulate_diff("Propensity", 0.5))
prog_mean_difs <- replicate(100, simulate_diff("Pilot", 0.5))
```


```{r}
difs_data <- data.frame(method = rep(c("Mahalanobis", "Propensity", "Joint"), each = 100),
                        mean_difference = c(mahal_mean_difs, prop_mean_difs, prog_mean_difs))

ggplot(difs_data, aes(x = method, y = mean_difference, fill = method)) +
  geom_boxplot() + 
  scale_fill_brewer(palette = "Set2") +
  ylab("Mean Prognostic Score Difference") +
  xlab("Method") +
  theme(legend.position = "none")
```

Here we see the mean prognostic score difference between treated and control individuals. I think this is really interesting.  Here we see that propensity score matching on average gives no systematic variation between the prognostic scores of matched individuals (boxplot centered at 0 = i.e. method is unbiased).  However, there is in practice a lot of variation between the prognostic scores of the matched individuals (method has high variance).  Prognostic score matching is centered close to zero (low bias) and shows very little variation (low variance). Some of the bias and variance of prognostic score matching would be avoided if we knew the prognostic model outright; in simulations where the prognostic model is already known perfectly, the box for Joint matchin is centered exactly at 0 and has very little variance.  Mahalanobis distance, however, systematically tends to match "sicker" treated individuals with "healthier" controls.  Even if the prognostic differences are lower on average than in propensity score matching, these differences are systematic, resulting in a biased estimate of treatment effect.


The two plots below are the same, except that $\rho$ is changed.  The first plot below has $\rho = 0$ (problem is unconfounded), and the second has $\rho = 0.9$ (problem is highly confounded by $X_1$).  The systematic difference in prognostic score from Mahalanobis distance is erased in the unconfounded data set and exacerbated in the highly confounded one.

```{r}
mahal_rho_0 <- replicate(100, simulate_diff("Mahalanobis", 0)) 
prop_rho_0 <- replicate(100, simulate_diff("Propensity", 0))
prog_rho_0 <- replicate(100, simulate_diff("Pilot", 0))
```


```{r}
difs_rho_0 <- data.frame(method = rep(c("Mahalanobis", "Propensity", "Joint"), each = 100),
                        mean_difference = c(mahal_rho_0, prop_rho_0, prog_rho_0))

ggplot(difs_rho_0, aes(x = method, y = mean_difference, fill = method)) +
  geom_boxplot() + 
  scale_fill_brewer(palette = "Set2") +
  ylab("Mean Prognostic Score Difference") +
  xlab("Method") +
  theme(legend.position = "none")
```

```{r}
mahal_rho_09 <- replicate(100, simulate_diff("Mahalanobis", 0.9)) 
prop_rho_09 <- replicate(100, simulate_diff("Propensity", 0.9))
prog_rho_09 <- replicate(100, simulate_diff("Pilot", 0.9))
```


```{r}
difs_rho_09 <- data.frame(method = rep(c("Mahalanobis", "Propensity", "Joint"), each = 100),
                        mean_difference = c(mahal_rho_09, prop_rho_09, prog_rho_09))

ggplot(difs_rho_09, aes(x = method, y = mean_difference, fill = method)) +
  geom_boxplot() + 
  scale_fill_brewer(palette = "Set2") +
  ylab("Mean Prognostic Score Difference") +
  xlab("Method") +
  theme(legend.position = "none")
```

```{r}
mahal_rho_0_nu_05 <- replicate(100, simulate_diff("Mahalanobis", rho = 0, nu = 0.5)) 
prop_rho_0_nu_05 <- replicate(100, simulate_diff("Propensity", rho = 0, nu = 0.5))
prog_rho_0_nu_05 <- replicate(100, simulate_diff("Pilot", rho = 0, nu = 0.5))
```


```{r}
difs_rho_0_nu_05 <- data.frame(method = rep(c("Mahalanobis", "Propensity", "Joint"), each = 100),
                        mean_difference = c(mahal_rho_0_nu_05, prop_rho_0_nu_05, prog_rho_0_nu_05))

ggplot(difs_rho_0_nu_05, aes(x = method, y = mean_difference, fill = method)) +
  geom_boxplot() + 
  scale_fill_brewer(palette = "Set2") +
  ylab("Mean Prognostic Score Difference") +
  xlab("Method") +
  theme(legend.position = "none")
```

# Future Directions

What this may be getting at is the larger question: "What defines a good matched set?" All three methods considered here work very well under the assumption that a perfect match can be found for each treated individual.  In reality, matches are almost always "wrong" by some amount.  The question is: how do we judge between imperfect matches to select the one most likely to give us a correct answer? 

Here are some things we can consider trying:

- Restrict the problem to 1:1 matching for simplicity.

- Fit propensity and prognostic scores with a lasso to try and reduce the problem of model fitting.

- Make a literature search to see who else has thought about this problem.  People to check: Sam Pimentel, Jas Sekhon, Kosuke Imai, Jose Zubizarreta, Ben Hansen